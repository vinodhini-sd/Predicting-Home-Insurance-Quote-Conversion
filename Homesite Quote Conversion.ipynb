# -*- coding: utf-8 -*-
"""Asmt3_v1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1TGQ6DPMJKwRpoUczf_P6pv67o4NN7bwD
"""

# Commented out IPython magic to ensure Python compatibility.
# To upload our datasets from our working directory we need to mount our drive contents to the colab environment. 
# For the code to do so you can search “mount” in code snippets or use the code given below. 
# Our entire drive contents are now mounted on colab at the location “/gdrive”.

from google.colab import drive
drive.mount('/gdrive')
#Change current working directory to gdrive
# %cd /gdrive

!pip install vecstack
!pip install imblearn

from vecstack import stacking
import pandas as pd
import numpy as np
from sklearn.metrics import accuracy_score #works
#from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
#from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import RandomizedSearchCV
from sklearn.model_selection import cross_val_score
#from sklearn.metrics import classification_report, confusion_matrix
#from sklearn.ensemble import GradientBoostingClassifier
from sklearn.neural_network import MLPClassifier
from imblearn.over_sampling import SMOTE 
from sklearn.svm import LinearSVC
from collections import Counter #for Smote, 
from sklearn.neighbors import KNeighborsClassifier

import warnings
warnings.filterwarnings("ignore")

#SPLIT TEST AND TRAIN DATA

#trainfile = r'C:/Users/Vinodhini/CIS 508/Train-modified.csv'
trainfile = r'/gdrive/My Drive/CIS_508/Colab Notebooks/Asmt3/Train-Modified.csv'
train_data = pd.read_csv(trainfile)

testfile = r'/gdrive/My Drive/CIS_508/Colab Notebooks/Asmt3/Test-Modified.csv'
test_data = pd.read_csv(testfile)

print(train_data.shape)
#print(train_data.head()) 

print(test_data.shape)
#print(test_data.head())

#SPLIT Xs AND Ys in the train and test data

categoricalFeatures = list(train_data.columns)
X_train = train_data.drop(columns = ['QuoteConversion_Flag']).copy(deep = True)
Y_train = train_data['QuoteConversion_Flag']
X_test = test_data.copy(deep = True)
print(X_train.shape)
print(Y_train.shape)
print(X_test.shape)

#SMOTE==============================================================================

print("___________________________________________________________________\nSMOTE\n")
print('Original dataset shape %s' % Counter(Y_train))
sm = SMOTE(sampling_strategy='float', ratio=0.5)
X_res, Y_res = sm.fit_resample(X_train, Y_train)
print('Resampled dataset shape %s' % Counter(Y_res))

#DECISION TREE =====================================================

#Default model
clf = DecisionTreeClassifier()
clf.fit(X_train, Y_train)
clf_predict=pd.DataFrame(clf.predict(X_test), columns = ['TARGET'])

#Hyperparameter tuning done for decision tree classifier
parameters={'min_samples_split' : range(10,30,10),'max_depth': range(1,5,2)}
clf_random = RandomizedSearchCV(clf,parameters,n_iter=5)
clf_random.fit(X_train, Y_train)
grid_parm=clf_random.best_params_
print(grid_parm)

#Using the parameters obtained from HyperParameterTuning in the DecisionTreeClassifier 
clf = DecisionTreeClassifier(**grid_parm)
clf.fit(X_train,Y_train)
clf_predict = pd.DataFrame(clf.predict(X_test))

#run cross-validation on best hyperparameters, get auc score
clf_cv_score = cross_val_score(clf, X_train, Y_train, cv=3, scoring="roc_auc")
print("=== All AUC Scores ===")
print(clf_cv_score)
print('\n')
print("=== Mean AUC Score ===")
print("Mean AUC Score - Decision Tree: ",clf_cv_score.mean())

#DecisionTreeClassifier using SMOTE data
clf1 = DecisionTreeClassifier(**grid_parm)
clf1.fit(X_res,Y_res)
clf_predict1 = pd.DataFrame(clf.predict(X_test))

#run cross-validation on best hyperparameters, get auc score - using SMOTE data
clf_cv_score1 = cross_val_score(clf1, X_res, Y_res, cv=3, scoring="roc_auc")
print("=== All AUC Scores ===")
print(clf_cv_score1)
print('\n')
print("=== Mean AUC Score ===")
print("Mean AUC Score - Decision Tree: ",clf_cv_score1.mean())

f1 = r'/gdrive/My Drive/CIS_508/Colab Notebooks/Asmt3/DT_Results.csv'
pd.concat([X_test['QuoteNumber'], clf_predict1], axis = 1).to_csv(f1, index=None)

#RANDOM FOREST =============================================================

#Default model
rfc = RandomForestClassifier()
rfc.fit(X_train, Y_train)
rfc_predict=pd.DataFrame(rfc.predict(X_test))

#Hyperparameter tuning for random forest
parameters={ 'n_estimators': range(50,150,50),'min_samples_split' : range(10,30,10),'max_depth': range(1,5,2)}
rfc_random = RandomizedSearchCV(rfc,parameters,n_iter=3)
rfc_random.fit(X_train, Y_train)
grid_parm_rfc=rfc_random.best_params_
print(grid_parm_rfc)

#contruct random forest using the best parameters
rfc= RandomForestClassifier(**grid_parm_rfc)
rfc.fit(X_train,Y_train)
rfc_predict = pd.DataFrame(rfc.predict(X_test))

#run cross-validation on best parameters, get auc score
rfc_cv_score = cross_val_score(rfc, X_train, Y_train, cv=3, scoring="roc_auc")
print("=== All AUC Scores ===")
print(rfc_cv_score)
print('\n')
print("=== Mean AUC Score ===")
print("Mean AUC Score - Random Forest: ",rfc_cv_score.mean())

#contruct random forest using the best parameters using SMOTE data
rfc1 = RandomForestClassifier(**grid_parm_rfc)
rfc1.fit(X_res,Y_res)
rfc_predict1 = pd.DataFrame(rfc1.predict(X_test))

#run cross-validation on best parameters, get auc score - using SMOTE Data
rfc_cv_score1 = cross_val_score(rfc1, X_res, Y_res, cv=3, scoring="roc_auc")
print("=== All AUC Scores ===")
print(rfc_cv_score1)
print('\n')
print("=== Mean AUC Score ===")
print("Mean AUC Score - Random Forest: ",rfc_cv_score1.mean())

f2 = r'/gdrive/My Drive/CIS_508/Colab Notebooks/Asmt3/RF_Results.csv'
pd.concat([X_test['QuoteNumber'], rfc_predict1], axis = 1).to_csv(f2, index=None)

#SUPPORT VECTOR MACHINE using Linear SVC====================================================

#simple SVC
clf_svc = LinearSVC()
clf_svc.fit(X_train, Y_train)
clf_svc_predict = pd.DataFrame(clf_svc.predict(X_test))

#SVC using SMOTE data
clf_svc1 = LinearSVC()
clf_svc1.fit(X_res, Y_res)
clf_svc_predict1 = pd.DataFrame(clf_svc1.predict(X_test))

#Hyperparameter tuning for SVM
parameters={ 'class_weight':['balanced'], 'max_iter':range(99,600,200)}
clf_svc2 = RandomizedSearchCV(clf_svc,parameters,n_iter=3)
clf_svc2.fit(X_train, Y_train)
grid_parm_rfc=clf_svc2.best_params_
print(grid_parm_rfc)

#contruct random forest using the best parameters
clf_svc3= LinearSVC(**grid_parm_rfc)
clf_svc3.fit(X_train,Y_train)
clf_svc_predict2 = pd.DataFrame(clf_svc3.predict(X_test))

f3 = r'/gdrive/My Drive/CIS_508/Colab Notebooks/Asmt3/SVM_Results.csv'
pd.concat([X_test['QuoteNumber'], clf_svc_predict1], axis = 1).to_csv(f3, index=None)

# MULTI LAYER PERCEPTRON =================================================

#simple mlp
clf_mlp = MLPClassifier(hidden_layer_sizes=3, alpha=0.1, random_state=0)
clf_mlp.fit(X_train, Y_train)
clf_mlp_predict = pd.DataFrame(clf_mlp.predict(X_test))

#MLP using SMOTE data
clf_mlp1 = MLPClassifier(hidden_layer_sizes=3, alpha=0.1, random_state=0)
clf_mlp1.fit(X_res, Y_res)
clf_mlp_predict1 = pd.DataFrame(clf_mlp1.predict(X_test))

parameters = {'hidden_layer_sizes':[(7,), (3,3), (2,2,2)] , 'alpha':[0.01,0.1,0.02]}
clf_mlp2 = RandomizedSearchCV(clf_mlp1,parameters,n_iter=3)
clf_mlp2.fit(X_train, Y_train)
grid_parm_rfc=clf_mlp2.best_params_
print(grid_parm_rfc)

#contruct random forest using the best parameters
clf_mlp3= MLPClassifier(**grid_parm_rfc)
clf_mlp3.fit(X_train,Y_train)
clf_mlp_predict2 = pd.DataFrame(clf_mlp3.predict(X_test))

f4 = r'/gdrive/My Drive/CIS_508/Colab Notebooks/Asmt3/MLP_Results.csv'
pd.concat([X_test['QuoteNumber'], clf_mlp_predict2], axis = 1).to_csv(f4, index=None)

# K-NEAREST NEIGHBOR  =======================================================

#simple knn
clf_knn = KNeighborsClassifier(n_neighbors = 5)
clf_knn.fit(X_train, Y_train)
clf_knn_predict = pd.DataFrame(clf_knn.predict(X_test))

#KNN using SMOTE data
clf_knn1 = KNeighborsClassifier(n_neighbors = 5)
clf_knn1.fit(X_res, Y_res)
clf_knn_predict1 = pd.DataFrame(clf_knn1.predict(X_test))

parameters = {'n_neighbors':range(1,6,2), 'leaf_size':range(10,30,20), 'p':range(1,3,1)}
clf_knn2 = RandomizedSearchCV(clf_knn1,parameters,n_iter=3)
clf_knn2.fit(X_train, Y_train)
grid_parm_rfc=clf_knn2.best_params_
print(grid_parm_rfc)

#contruct random forest using the best parameters
clf_knn3= KNeighborsClassifier(**grid_parm_rfc)
clf_knn3.fit(X_train,Y_train)
clf_knn_predict2 = pd.DataFrame(clf_knn3.predict(X_test))

f5 = r'/gdrive/My Drive/CIS_508/Colab Notebooks/Asmt3/KNN_Results.csv'
pd.concat([X_test['QuoteNumber'], clf_knn_predict2], axis = 1).to_csv(f5, index=None)

#STACKING MODELS =====================================================================
print("___________________________________________________________________________________________\nEnsemble Methods Predictions using Random Forest, SVM, Neural Net, KNN and Decision Tree Classifier\n")

models = [ RandomForestClassifier(n_estimators=100,min_samples_split=10, max_depth=3), DecisionTreeClassifier(min_samples_split= 10, max_depth=3), LinearSVC(), MLPClassifier(hidden_layer_sizes=3, alpha=0.1, random_state=0), KNeighborsClassifier(n_neighbors = 5) ]
      
S_Train, S_Test = stacking(models,                   
                           X_res, Y_res, X_test,   
                           regression=False, 
     
                           mode='oof_pred_bag', 
       
                           needs_proba=False,
         
                           save_dir=None,

                           metric=accuracy_score, 
    
                           n_folds=4, 
                 
                           stratified=True,
            
                           shuffle=True,  
            
                           random_state=0,    
         
                           verbose=2)
print(S_Train.shape)
print(S_Test.shape)

#STACKING - CONTRUCT A RANDOM FOREST CLASSIFIER MODEL==============================
model = RandomForestClassifier()
    
model = model.fit(S_Train, Y_res)
y_pred = pd.DataFrame(model.predict(S_Test),columns=['QuoteConversion_Flag'])

f6 = r'/gdrive/My Drive/CIS_508/Colab Notebooks/Asmt3/Stacking_Results.csv'
pd.concat([X_test['QuoteNumber'], y_pred], axis = 1).to_csv(fileNameKag, index=None)

testY_pred_file_kag = pd.read_csv(fileNameKag)
testY_pred_file_kag.head()









